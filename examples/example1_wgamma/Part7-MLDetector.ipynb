{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first load all the python libraries again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "madminer_src_path = \"/Users/felixkling/Documents/GitHub/madminer\"\n",
    "sys.path.append(madminer_src_path)\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "% matplotlib inline\n",
    "\n",
    "from madminer.delphes import DelphesProcessor\n",
    "from madminer.sampling import combine_and_shuffle\n",
    "from madminer.utils.particle import MadMinerParticle\n",
    "\n",
    "from madminer.fisherinformation import FisherInformation\n",
    "from madminer.fisherinformation import project_information,profile_information\n",
    "\n",
    "from madminer.plotting import plot_fisher_information_contours_2d\n",
    "\n",
    "from madminer.sampling import SampleAugmenter\n",
    "from madminer.sampling import constant_benchmark_theta, multiple_benchmark_thetas\n",
    "from madminer.sampling import constant_morphing_theta, multiple_morphing_thetas, random_morphing_thetas\n",
    "from madminer.ml import MLForge, EnsembleForge\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run MadMiner at Detector Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define the input file, the number of samples and *effective* number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhedatafile = 'data/madminer_lhedata.h5'\n",
    "detectordatafile='data/madminer_detectordata.h5'\n",
    "pythiadatafile='data/madminer_pythiadata.h5'\n",
    "\n",
    "nsamples = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10a) Run the Data Augmentation and Machine Learning part with Detetector Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we once again augment the data and machine learning part again. Here  `n_samples` should be choosen similar to the effective number of events, which also depends on the cuts choosen earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:01  \n",
      "10:01  ------------------------------------------------------------\n",
      "10:01  |                                                          |\n",
      "10:01  |  MadMiner v0.1.1                                         |\n",
      "10:01  |                                                          |\n",
      "10:01  |           Johann Brehmer, Kyle Cranmer, and Felix Kling  |\n",
      "10:01  |                                                          |\n",
      "10:01  ------------------------------------------------------------\n",
      "10:01  \n",
      "10:01  Loading data from data/madminer_detectordata.h5\n",
      "10:01  Found 2 parameters:\n",
      "10:01     CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:01     CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:01  Found 6 benchmarks:\n",
      "10:01     sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00\n",
      "10:01     w: CWL2 = 20.00, CPWL2 = 0.00e+00\n",
      "10:01     morphing_basis_vector_2: CWL2 = -3.26e+01, CPWL2 = -4.46e+01\n",
      "10:01     morphing_basis_vector_3: CWL2 = 8.14, CPWL2 = -3.50e+01\n",
      "10:01     morphing_basis_vector_4: CWL2 = -3.51e+01, CPWL2 = 32.41\n",
      "10:01     morphing_basis_vector_5: CWL2 = 5.86, CPWL2 = 39.09\n",
      "10:01  Found 21 observables: px_l, px_v, px_a, py_l, py_v, py_a, pz_l, pz_a, e_l, e_a, pt_l, pt_v, pt_a, eta_l, eta_a, dphi_lv, dphi_la, dphi_va, m_la, mt, phi_resurrection\n",
      "10:01  Found 77552 events\n",
      "10:01  Found morphing setup with 6 components\n",
      "10:01  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:01  Effective number of samples: 38796.80963976638\n",
      "10:01  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:01  Effective number of samples: 38796.80963976638\n",
      "10:01  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:01  Effective number of samples: 38796.80963976638\n",
      "10:01  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:01  Effective number of samples: 38796.80963976638\n",
      "10:01  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:01  Effective number of samples: 38796.80963976638\n",
      "10:01  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:01  Effective number of samples: 38795.80853971576\n"
     ]
    }
   ],
   "source": [
    "sa = SampleAugmenter(detectordatafile, debug=False)\n",
    "\n",
    "n_estimators = 5\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    x, theta, t_xz = sa.extract_samples_train_local(\n",
    "        theta=constant_benchmark_theta('sm'),\n",
    "        n_samples=int(nsamples/2),\n",
    "        folder='./data/samples_detector/',\n",
    "        filename='train{}'.format(i)\n",
    "    )\n",
    "\n",
    "x, theta, t_xz = sa.extract_samples_train_local(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=int(nsamples/2),\n",
    "    folder='./data/samples_detector/',\n",
    "    filename='test',\n",
    "    switch_train_test_events=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the perform the ML part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:01  Training 5 estimators in ensemble\n",
      "10:01  Training estimator 1 / 5 in ensemble\n",
      "10:01  Starting training\n",
      "10:01    Method:                 sally\n",
      "10:01    Training data: x at data/samples_detector/x_train0.npy\n",
      "10:01                   t_xz (theta0) at  data/samples_detector/t_xz_train0.npy\n",
      "10:01    Features:               all\n",
      "10:01    Method:                 sally\n",
      "10:01    Hidden layers:          (100, 100, 100, 100)\n",
      "10:01    Activation function:    tanh\n",
      "10:01    Batch size:             128\n",
      "10:01    Trainer:                amsgrad\n",
      "10:01    Epochs:                 100\n",
      "10:01    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:01    Validation split:       None\n",
      "10:01    Early stopping:         True\n",
      "10:01    Scale inputs:           True\n",
      "10:01    Shuffle labels          False\n",
      "10:01    Regularization:         None\n",
      "10:01  Loading training data\n",
      "10:01  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:01  Rescaling inputs\n",
      "10:01  Creating model for method sally\n",
      "10:01  Training model\n",
      "10:01    Epoch 10: train loss 0.0006 (mse_score: 0.0006)\n",
      "10:01    Epoch 20: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:01    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:02    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:02    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:02    Epoch 60: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:03    Epoch 70: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:03    Epoch 80: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:03    Epoch 90: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:03    Epoch 100: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:03  Finished training\n",
      "10:03  Training estimator 2 / 5 in ensemble\n",
      "10:03  Starting training\n",
      "10:03    Method:                 sally\n",
      "10:03    Training data: x at data/samples_detector/x_train1.npy\n",
      "10:03                   t_xz (theta0) at  data/samples_detector/t_xz_train1.npy\n",
      "10:03    Features:               all\n",
      "10:03    Method:                 sally\n",
      "10:03    Hidden layers:          (100, 100, 100, 100)\n",
      "10:03    Activation function:    tanh\n",
      "10:03    Batch size:             128\n",
      "10:03    Trainer:                amsgrad\n",
      "10:03    Epochs:                 100\n",
      "10:03    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:03    Validation split:       None\n",
      "10:03    Early stopping:         True\n",
      "10:03    Scale inputs:           True\n",
      "10:03    Shuffle labels          False\n",
      "10:03    Regularization:         None\n",
      "10:03  Loading training data\n",
      "10:03  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:03  Rescaling inputs\n",
      "10:03  Creating model for method sally\n",
      "10:03  Training model\n",
      "10:04    Epoch 10: train loss 0.0007 (mse_score: 0.0007)\n",
      "10:04    Epoch 20: train loss 0.0006 (mse_score: 0.0006)\n",
      "10:04    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:04    Epoch 40: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:04    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:05    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:05    Epoch 70: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:05    Epoch 80: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:05    Epoch 90: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:06    Epoch 100: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:06  Finished training\n",
      "10:06  Training estimator 3 / 5 in ensemble\n",
      "10:06  Starting training\n",
      "10:06    Method:                 sally\n",
      "10:06    Training data: x at data/samples_detector/x_train2.npy\n",
      "10:06                   t_xz (theta0) at  data/samples_detector/t_xz_train2.npy\n",
      "10:06    Features:               all\n",
      "10:06    Method:                 sally\n",
      "10:06    Hidden layers:          (100, 100, 100, 100)\n",
      "10:06    Activation function:    tanh\n",
      "10:06    Batch size:             128\n",
      "10:06    Trainer:                amsgrad\n",
      "10:06    Epochs:                 100\n",
      "10:06    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:06    Validation split:       None\n",
      "10:06    Early stopping:         True\n",
      "10:06    Scale inputs:           True\n",
      "10:06    Shuffle labels          False\n",
      "10:06    Regularization:         None\n",
      "10:06  Loading training data\n",
      "10:06  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:06  Rescaling inputs\n",
      "10:06  Creating model for method sally\n",
      "10:06  Training model\n",
      "10:06    Epoch 10: train loss 0.0007 (mse_score: 0.0007)\n",
      "10:06    Epoch 20: train loss 0.0006 (mse_score: 0.0006)\n",
      "10:06    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:07    Epoch 40: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:07    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:07    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:08    Epoch 70: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:08    Epoch 80: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:08    Epoch 90: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:08    Epoch 100: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:08  Finished training\n",
      "10:08  Training estimator 4 / 5 in ensemble\n",
      "10:08  Starting training\n",
      "10:08    Method:                 sally\n",
      "10:08    Training data: x at data/samples_detector/x_train3.npy\n",
      "10:08                   t_xz (theta0) at  data/samples_detector/t_xz_train3.npy\n",
      "10:08    Features:               all\n",
      "10:08    Method:                 sally\n",
      "10:08    Hidden layers:          (100, 100, 100, 100)\n",
      "10:08    Activation function:    tanh\n",
      "10:08    Batch size:             128\n",
      "10:08    Trainer:                amsgrad\n",
      "10:08    Epochs:                 100\n",
      "10:08    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:08    Validation split:       None\n",
      "10:08    Early stopping:         True\n",
      "10:08    Scale inputs:           True\n",
      "10:08    Shuffle labels          False\n",
      "10:08    Regularization:         None\n",
      "10:08  Loading training data\n",
      "10:08  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:08  Rescaling inputs\n",
      "10:08  Creating model for method sally\n",
      "10:08  Training model\n",
      "10:09    Epoch 10: train loss 0.0007 (mse_score: 0.0007)\n",
      "10:09    Epoch 20: train loss 0.0006 (mse_score: 0.0006)\n",
      "10:09    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:09    Epoch 40: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:09    Epoch 50: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:10    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:10    Epoch 70: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:10    Epoch 80: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:10    Epoch 90: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:11    Epoch 100: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:11  Finished training\n",
      "10:11  Training estimator 5 / 5 in ensemble\n",
      "10:11  Starting training\n",
      "10:11    Method:                 sally\n",
      "10:11    Training data: x at data/samples_detector/x_train4.npy\n",
      "10:11                   t_xz (theta0) at  data/samples_detector/t_xz_train4.npy\n",
      "10:11    Features:               all\n",
      "10:11    Method:                 sally\n",
      "10:11    Hidden layers:          (100, 100, 100, 100)\n",
      "10:11    Activation function:    tanh\n",
      "10:11    Batch size:             128\n",
      "10:11    Trainer:                amsgrad\n",
      "10:11    Epochs:                 100\n",
      "10:11    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:11    Validation split:       None\n",
      "10:11    Early stopping:         True\n",
      "10:11    Scale inputs:           True\n",
      "10:11    Shuffle labels          False\n",
      "10:11    Regularization:         None\n",
      "10:11  Loading training data\n",
      "10:11  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:11  Rescaling inputs\n",
      "10:11  Creating model for method sally\n",
      "10:11  Training model\n",
      "10:11    Epoch 10: train loss 0.0007 (mse_score: 0.0007)\n",
      "10:11    Epoch 20: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:11    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:12    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:12    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:12    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:12    Epoch 70: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:12    Epoch 80: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:13    Epoch 90: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:13    Epoch 100: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:13  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble = EnsembleForge(estimators=n_estimators)\n",
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x_filename=['data/samples_detector/x_train{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=['data/samples_detector/t_xz_train{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    \n",
    "    n_epochs=100,\n",
    "    n_hidden=(100,100,100,100),\n",
    "    activation='tanh',\n",
    "    initial_lr=0.001,\n",
    "    final_lr=0.0001\n",
    ")\n",
    "\n",
    "ensemble.save('models/samples_detector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10b) Run the Data Augmentation and Machine Learning part with Pythia Level Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repeat the same procedure, but with the pythia level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:13  Loading data from data/madminer_pythiadata.h5\n",
      "10:13  Found 2 parameters:\n",
      "10:13     CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:13     CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:13  Found 6 benchmarks:\n",
      "10:13     sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00\n",
      "10:13     w: CWL2 = 20.00, CPWL2 = 0.00e+00\n",
      "10:13     morphing_basis_vector_2: CWL2 = -3.26e+01, CPWL2 = -4.46e+01\n",
      "10:13     morphing_basis_vector_3: CWL2 = 8.14, CPWL2 = -3.50e+01\n",
      "10:13     morphing_basis_vector_4: CWL2 = -3.51e+01, CPWL2 = 32.41\n",
      "10:13     morphing_basis_vector_5: CWL2 = 5.86, CPWL2 = 39.09\n",
      "10:13  Found 21 observables: px_l, px_v, px_a, py_l, py_v, py_a, pz_l, pz_a, e_l, e_a, pt_l, pt_v, pt_a, eta_l, eta_a, dphi_lv, dphi_la, dphi_va, m_la, mt, phi_resurrection\n",
      "10:13  Found 77552 events\n",
      "10:13  Found morphing setup with 6 components\n",
      "10:13  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:13  Effective number of samples: 38796.80963976638\n",
      "10:13  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:13  Effective number of samples: 38796.80963976638\n",
      "10:13  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:13  Effective number of samples: 38796.80963976638\n",
      "10:13  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:13  Effective number of samples: 38796.80963976638\n",
      "10:13  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:13  Effective number of samples: 38796.80963976638\n",
      "10:13  Extracting training sample for local score regression. Sampling and score evaluation according to (u'benchmark', u'sm')\n",
      "10:13  Effective number of samples: 38795.80853971576\n"
     ]
    }
   ],
   "source": [
    "sa = SampleAugmenter(pythiadatafile, debug=False)\n",
    "\n",
    "n_estimators = 5\n",
    "\n",
    "for i in range(n_estimators):\n",
    "    x, theta, t_xz = sa.extract_samples_train_local(\n",
    "        theta=constant_benchmark_theta('sm'),\n",
    "        n_samples=int(nsamples/2),\n",
    "        folder='./data/samples_pythia/',\n",
    "        filename='train{}'.format(i)\n",
    "    )\n",
    "\n",
    "x, theta, t_xz = sa.extract_samples_train_local(\n",
    "    theta=constant_benchmark_theta('sm'),\n",
    "    n_samples=int(nsamples/2),\n",
    "    folder='./data/samples_pythia/',\n",
    "    filename='test',\n",
    "    switch_train_test_events=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:13  Training 5 estimators in ensemble\n",
      "10:13  Training estimator 1 / 5 in ensemble\n",
      "10:13  Starting training\n",
      "10:13    Method:                 sally\n",
      "10:13    Training data: x at data/samples_pythia/x_train0.npy\n",
      "10:13                   t_xz (theta0) at  data/samples_pythia/t_xz_train0.npy\n",
      "10:13    Features:               all\n",
      "10:13    Method:                 sally\n",
      "10:13    Hidden layers:          (100, 100, 100, 100)\n",
      "10:13    Activation function:    tanh\n",
      "10:13    Batch size:             128\n",
      "10:13    Trainer:                amsgrad\n",
      "10:13    Epochs:                 100\n",
      "10:13    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:13    Validation split:       None\n",
      "10:13    Early stopping:         True\n",
      "10:13    Scale inputs:           True\n",
      "10:13    Shuffle labels          False\n",
      "10:13    Regularization:         None\n",
      "10:13  Loading training data\n",
      "10:13  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:13  Rescaling inputs\n",
      "10:13  Creating model for method sally\n",
      "10:13  Training model\n",
      "10:13    Epoch 10: train loss 0.0007 (mse_score: 0.0007)\n",
      "10:13    Epoch 20: train loss 0.0006 (mse_score: 0.0006)\n",
      "10:13    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:14    Epoch 40: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:14    Epoch 50: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:14    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:14    Epoch 70: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:15    Epoch 80: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:15    Epoch 90: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:15    Epoch 100: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:15  Finished training\n",
      "10:15  Training estimator 2 / 5 in ensemble\n",
      "10:15  Starting training\n",
      "10:15    Method:                 sally\n",
      "10:15    Training data: x at data/samples_pythia/x_train1.npy\n",
      "10:15                   t_xz (theta0) at  data/samples_pythia/t_xz_train1.npy\n",
      "10:15    Features:               all\n",
      "10:15    Method:                 sally\n",
      "10:15    Hidden layers:          (100, 100, 100, 100)\n",
      "10:15    Activation function:    tanh\n",
      "10:15    Batch size:             128\n",
      "10:15    Trainer:                amsgrad\n",
      "10:15    Epochs:                 100\n",
      "10:15    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:15    Validation split:       None\n",
      "10:15    Early stopping:         True\n",
      "10:15    Scale inputs:           True\n",
      "10:15    Shuffle labels          False\n",
      "10:15    Regularization:         None\n",
      "10:15  Loading training data\n",
      "10:15  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:15  Rescaling inputs\n",
      "10:15  Creating model for method sally\n",
      "10:15  Training model\n",
      "10:15    Epoch 10: train loss 0.0007 (mse_score: 0.0007)\n",
      "10:15    Epoch 20: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:16    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:16    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:16    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:16    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:16    Epoch 70: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:17    Epoch 80: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:17    Epoch 90: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:17    Epoch 100: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:17  Finished training\n",
      "10:17  Training estimator 3 / 5 in ensemble\n",
      "10:17  Starting training\n",
      "10:17    Method:                 sally\n",
      "10:17    Training data: x at data/samples_pythia/x_train2.npy\n",
      "10:17                   t_xz (theta0) at  data/samples_pythia/t_xz_train2.npy\n",
      "10:17    Features:               all\n",
      "10:17    Method:                 sally\n",
      "10:17    Hidden layers:          (100, 100, 100, 100)\n",
      "10:17    Activation function:    tanh\n",
      "10:17    Batch size:             128\n",
      "10:17    Trainer:                amsgrad\n",
      "10:17    Epochs:                 100\n",
      "10:17    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:17    Validation split:       None\n",
      "10:17    Early stopping:         True\n",
      "10:17    Scale inputs:           True\n",
      "10:17    Shuffle labels          False\n",
      "10:17    Regularization:         None\n",
      "10:17  Loading training data\n",
      "10:17  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:17  Rescaling inputs\n",
      "10:17  Creating model for method sally\n",
      "10:17  Training model\n",
      "10:17    Epoch 10: train loss 0.0008 (mse_score: 0.0008)\n",
      "10:18    Epoch 20: train loss 0.0006 (mse_score: 0.0006)\n",
      "10:18    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:18    Epoch 40: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:18    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:18    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:19    Epoch 70: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:19    Epoch 80: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:19    Epoch 90: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:19    Epoch 100: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:19  Finished training\n",
      "10:19  Training estimator 4 / 5 in ensemble\n",
      "10:19  Starting training\n",
      "10:19    Method:                 sally\n",
      "10:19    Training data: x at data/samples_pythia/x_train3.npy\n",
      "10:19                   t_xz (theta0) at  data/samples_pythia/t_xz_train3.npy\n",
      "10:19    Features:               all\n",
      "10:19    Method:                 sally\n",
      "10:19    Hidden layers:          (100, 100, 100, 100)\n",
      "10:19    Activation function:    tanh\n",
      "10:19    Batch size:             128\n",
      "10:19    Trainer:                amsgrad\n",
      "10:19    Epochs:                 100\n",
      "10:19    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:19    Validation split:       None\n",
      "10:19    Early stopping:         True\n",
      "10:19    Scale inputs:           True\n",
      "10:19    Shuffle labels          False\n",
      "10:19    Regularization:         None\n",
      "10:19  Loading training data\n",
      "10:19  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:19  Rescaling inputs\n",
      "10:19  Creating model for method sally\n",
      "10:19  Training model\n",
      "10:20    Epoch 10: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:20    Epoch 20: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:20    Epoch 30: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:20    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:20    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:21    Epoch 60: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:21    Epoch 70: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:21    Epoch 80: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:21    Epoch 90: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:21    Epoch 100: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:21  Finished training\n",
      "10:21  Training estimator 5 / 5 in ensemble\n",
      "10:21  Starting training\n",
      "10:21    Method:                 sally\n",
      "10:21    Training data: x at data/samples_pythia/x_train4.npy\n",
      "10:21                   t_xz (theta0) at  data/samples_pythia/t_xz_train4.npy\n",
      "10:21    Features:               all\n",
      "10:21    Method:                 sally\n",
      "10:21    Hidden layers:          (100, 100, 100, 100)\n",
      "10:21    Activation function:    tanh\n",
      "10:21    Batch size:             128\n",
      "10:21    Trainer:                amsgrad\n",
      "10:21    Epochs:                 100\n",
      "10:21    Learning rate:          0.001 initially, decaying to 0.0001\n",
      "10:21    Validation split:       None\n",
      "10:21    Early stopping:         True\n",
      "10:21    Scale inputs:           True\n",
      "10:21    Shuffle labels          False\n",
      "10:21    Regularization:         None\n",
      "10:21  Loading training data\n",
      "10:21  Found 50000 samples with 2 parameters and 21 observables\n",
      "10:21  Rescaling inputs\n",
      "10:21  Creating model for method sally\n",
      "10:21  Training model\n",
      "10:22    Epoch 10: train loss 0.0007 (mse_score: 0.0007)\n",
      "10:22    Epoch 20: train loss 0.0006 (mse_score: 0.0006)\n",
      "10:22    Epoch 30: train loss 0.0005 (mse_score: 0.0005)\n",
      "10:22    Epoch 40: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:22    Epoch 50: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:23    Epoch 60: train loss 0.0004 (mse_score: 0.0004)\n",
      "10:23    Epoch 70: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:23    Epoch 80: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:23    Epoch 90: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:23    Epoch 100: train loss 0.0003 (mse_score: 0.0003)\n",
      "10:23  Finished training\n"
     ]
    }
   ],
   "source": [
    "ensemble = EnsembleForge(estimators=n_estimators)\n",
    "ensemble.train_all(\n",
    "    method='sally',\n",
    "    x_filename=['data/samples_pythia/x_train{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    t_xz0_filename=['data/samples_pythia/t_xz_train{}.npy'.format(i) for i in range(n_estimators)],\n",
    "    \n",
    "    n_epochs=100,\n",
    "    n_hidden=(100,100,100,100),\n",
    "    activation='tanh',\n",
    "    initial_lr=0.001,\n",
    "    final_lr=0.0001\n",
    ")\n",
    "\n",
    "ensemble.save('models/samples_pythia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10c) Obtain the Fisher Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the Fisher Info again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10:23  Loading data from data/madminer_lhedata.h5\n",
      "10:23  Found 2 parameters:\n",
      "10:23     CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:23     CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:23  Found 6 benchmarks:\n",
      "10:23     sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00\n",
      "10:23     w: CWL2 = 20.00, CPWL2 = 0.00e+00\n",
      "10:23     morphing_basis_vector_2: CWL2 = -3.26e+01, CPWL2 = -4.46e+01\n",
      "10:23     morphing_basis_vector_3: CWL2 = 8.14, CPWL2 = -3.50e+01\n",
      "10:23     morphing_basis_vector_4: CWL2 = -3.51e+01, CPWL2 = 32.41\n",
      "10:23     morphing_basis_vector_5: CWL2 = 5.86, CPWL2 = 39.09\n",
      "10:23  Found 26 observables: px_l, px_v, px_a, py_l, py_v, py_a, pz_l, pz_v, pz_a, e_l, e_v, e_a, pt_l, pt_v, pt_a, eta_l, eta_v, eta_a, dphi_lv, dphi_la, dphi_va, m_lv, m_lva, m_la, mtrans, phi_resurrection\n",
      "10:23  Found 100000 events\n",
      "10:23  Found morphing setup with 6 components\n",
      "10:23  Evaluating rate Fisher information\n",
      "10:23  Found ensemble with 5 estimators and expectations None\n",
      "10:24  Evaluating rate Fisher information\n",
      "10:24  Found ensemble with 5 estimators and expectations None\n",
      "10:24  Loading data from data/madminer_detectordata.h5\n",
      "10:24  Found 2 parameters:\n",
      "10:24     CWL2 (LHA: dim6 2, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:24     CPWL2 (LHA: dim6 5, maximal power in squared ME: (2,), range: (-50.0, 50.0))\n",
      "10:24  Found 6 benchmarks:\n",
      "10:24     sm: CWL2 = 0.00e+00, CPWL2 = 0.00e+00\n",
      "10:24     w: CWL2 = 20.00, CPWL2 = 0.00e+00\n",
      "10:24     morphing_basis_vector_2: CWL2 = -3.26e+01, CPWL2 = -4.46e+01\n",
      "10:24     morphing_basis_vector_3: CWL2 = 8.14, CPWL2 = -3.50e+01\n",
      "10:24     morphing_basis_vector_4: CWL2 = -3.51e+01, CPWL2 = 32.41\n",
      "10:24     morphing_basis_vector_5: CWL2 = 5.86, CPWL2 = 39.09\n",
      "10:24  Found 21 observables: px_l, px_v, px_a, py_l, py_v, py_a, pz_l, pz_a, e_l, e_a, pt_l, pt_v, pt_a, eta_l, eta_a, dphi_lv, dphi_la, dphi_va, m_la, mt, phi_resurrection\n",
      "10:24  Found 77552 events\n",
      "10:24  Found morphing setup with 6 components\n",
      "10:24  Evaluating rate Fisher information\n"
     ]
    }
   ],
   "source": [
    "fisher_parton = FisherInformation(lhedatafile, debug=False)\n",
    "\n",
    "fi_ml_mean, fi_ml_covariance = fisher_parton.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/samples_ensemble',\n",
    "    unweighted_x_sample_file='data/samples_ensemble/x_test.npy',\n",
    "    luminosity=300*1000.\n",
    ")\n",
    "\n",
    "fi_metonly_mean, fi_metonly_covariance = fisher_parton.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/samples_metonly',\n",
    "    unweighted_x_sample_file='data/samples_ensemble/x_test.npy',\n",
    "    luminosity=300*1000.\n",
    ")\n",
    "\n",
    "fi_truth_mean, fi_truth_covariance = fisher_parton.calculate_fisher_information_full_truth(\n",
    "    theta=[0.,0.],\n",
    "    luminosity=300*1000.\n",
    ")\n",
    "\n",
    "fisher_detector = FisherInformation(detectordatafile, debug=False)\n",
    "\n",
    "fi_detector_mean, fi_detector_covariance = fisher_detector.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/samples_detector',\n",
    "    unweighted_x_sample_file='data/samples_detector/x_test.npy',\n",
    "    luminosity=300*1000.\n",
    ")\n",
    "\n",
    "fisher_pythia = FisherInformation(pythiadatafile, debug=False)\n",
    "\n",
    "fi_pythia_mean, fi_pythia_covariance = fisher_pythia.calculate_fisher_information_full_detector(\n",
    "    theta=[0.,0.],\n",
    "    model_file='models/samples_pythia',\n",
    "    unweighted_x_sample_file='data/samples_pythia/x_test.npy',\n",
    "    luminosity=300*1000.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_fisher_information_contours_2d(\n",
    "    [fi_ml_mean, fi_metonly_mean, fi_detector_mean,fi_pythia_mean,fi_truth_mean ],\n",
    "    [fi_ml_covariance, fi_metonly_covariance,fi_detector_covariance, fi_pythia_covariance,fi_truth_covariance],\n",
    "    colors=[u'C0',u'C1',u'C2',u'C3',\"black\"],\n",
    "    linestyles=[\"solid\",\"solid\",\"solid\",\"solid\",\"dashed\"],\n",
    "    inline_labels=[\"PL: all\",\"PL: MET\",\"Detector\",\"Pythia\",\"PL: truth\"],\n",
    "    xrange=(-15,15),\n",
    "    yrange=(-5,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
